{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69979aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786641ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0949e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download()\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212b763",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e865218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "dataset = [\n",
    "    {\n",
    "        \"scenario\": \"Product Sales\",\n",
    "        \"dialogues\": [\n",
    "            {\n",
    "                \"dialogue\": [\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Hello, this is Nandini from Amazon. How are you today?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"I'm good, thank you.\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Great to hear! I'm calling to introduce you to our latest software solution that can help streamline your business operations. Are you currently using any software for project management?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"Yes, we are using Chrome.\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"That's great! Our software integrates seamlessly with Chrome and offers additional features such as advanced analytics and real-time collaboration. Would you be interested in a demo?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"Sure, I would like to see a demo.\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Excellent! I can schedule a demo for you at your convenience. What time works best for you?\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"dialogue\": [\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Hi, this is Nandini from Amazon. How are you today?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"I'm fine, who is this?\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Iâ€™m with Amazon, and we have a new project management tool that I think you'll find very useful. Are you currently managing projects manually or using software?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"We do it manually.\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Our tool can automate a lot of the tasks you're doing manually, saving you time and reducing errors. Can I tell you more about how it works?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"Okay, I'm listening.\"},\n",
    "                    {\"speaker\": \"Agent\", \"text\": \"Great! Our software provides features like task automation, progress tracking, and team collaboration tools. Would you be interested in a free trial to see how it can benefit your work?\"},\n",
    "                    {\"speaker\": \"Customer\", \"text\": \"Yes, a free trial sounds good.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd75e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scenario': 'Product Sales',\n",
       "  'dialogues': [{'dialogue': [{'speaker': 'Agent',\n",
       "      'text': 'Hello, this is [Agent Name] from [Company Name]. How are you today?'},\n",
       "     {'speaker': 'Customer', 'text': \"I'm good, thank you.\"},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': \"Great to hear! I'm calling to introduce you to our latest software solution that can help streamline your business operations. Are you currently using any software for project management?\"},\n",
       "     {'speaker': 'Customer', 'text': 'Yes, we are using [Current Software].'},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': \"That's great! Our software integrates seamlessly with [Current Software] and offers additional features such as advanced analytics and real-time collaboration. Would you be interested in a demo?\"},\n",
       "     {'speaker': 'Customer', 'text': 'Sure, I would like to see a demo.'},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': 'Excellent! I can schedule a demo for you at your convenience. What time works best for you?'}]},\n",
       "   {'dialogue': [{'speaker': 'Agent',\n",
       "      'text': 'Hi, this is [Agent Name] from [Company Name]. How are you today?'},\n",
       "     {'speaker': 'Customer', 'text': \"I'm fine, who is this?\"},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': \"Iâ€™m with [Company Name], and we have a new project management tool that I think you'll find very useful. Are you currently managing projects manually or using software?\"},\n",
       "     {'speaker': 'Customer', 'text': 'We do it manually.'},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': \"Our tool can automate a lot of the tasks you're doing manually, saving you time and reducing errors. Can I tell you more about how it works?\"},\n",
       "     {'speaker': 'Customer', 'text': \"Okay, I'm listening.\"},\n",
       "     {'speaker': 'Agent',\n",
       "      'text': 'Great! Our software provides features like task automation, progress tracking, and team collaboration tools. Would you be interested in a free trial to see how it can benefit your work?'},\n",
       "     {'speaker': 'Customer', 'text': 'Yes, a free trial sounds good.'}]}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9f36b",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94a907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "for scenario in dataset:\n",
    "    for dialogue in scenario['dialogues']:\n",
    "        for turn in dialogue['dialogue']:\n",
    "            conversations.append(turn['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c7d8c",
   "metadata": {},
   "source": [
    "# Train the Dialogue model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21af2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b990ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\asus/.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\asus/.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\asus/.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\asus/.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\asus/.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium\\snapshots\\7b40bb0f92c45fefa957d088000d8648e5c7fa33\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc98ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning [PAD] to the pad_token key of the tokenizer\n",
      "Adding [PAD] to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1024)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add padding token \n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9335d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_texts = tokenize_function(conversations)\n",
    "\n",
    "# Convert to Dataset object\n",
    "dataset = Dataset.from_dict(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286af809",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokens = tokenizer(conversations, return_tensors='pt', truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbbfe49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5c64db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4da4de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e84f42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 354824192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 08:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=4.672073046366374, metrics={'train_runtime': 549.241, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.044, 'total_flos': 10447882813440.0, 'train_loss': 4.672073046366374, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48591fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./cold_calling_model\\config.json\n",
      "Model weights saved in ./cold_calling_model\\pytorch_model.bin\n",
      "tokenizer config file saved in ./cold_calling_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./cold_calling_model\\special_tokens_map.json\n",
      "added tokens file saved in ./cold_calling_model\\added_tokens.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./cold_calling_model\\\\tokenizer_config.json',\n",
       " './cold_calling_model\\\\special_tokens_map.json',\n",
       " './cold_calling_model\\\\vocab.json',\n",
       " './cold_calling_model\\\\merges.txt',\n",
       " './cold_calling_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenize\n",
    "model.save_pretrained(\"./cold_calling_model\")\n",
    "tokenizer.save_pretrained(\"./cold_calling_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c5b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "454138f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49104641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a new dialogue\n",
    "input_text = \"Hello, this is Nandini from Amazon. How are you today?\"\n",
    "generated_text = text_generator(input_text, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47e0be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is Nandini from Amazon. How are you today?\n"
     ]
    }
   ],
   "source": [
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150479a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285328b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
